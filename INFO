

net focuses
    fast training
        use overfitting and long term mem weights
            good init
            fast exploration
    better learning
        think about choice
    structure
        immediate mem --> overfits to recent experiences
        short term mem --> models local task / env
        long term mem --> learns all dependecies / patterns

net goals
    better learning
        learns why to do something
        learns when to trust immediate, short, long mem outputs

ideas
    one model (no reward input?)
        extract (action rewards, state reward?) from state
            defaults to length of time per episode if no reward given
                able to switch between learned, given, default
            self tunes to focus on that reward
        extract best action
            explore more if high entropy
        evolve self
            able to change structure and self tune
        predict step to convergence
            ie one large step then learn with small steps
            aka recognize activity and remember weights?
