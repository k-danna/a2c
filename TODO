
workflow
    part 1/2
        long term net recognizes weights for different activities
            generates weights for activities?
        short term net helps to explore
    parrt ?
        adversarial learning?
            make it a game for himself
        learn only from a small couple of games
            overfit hard to the first
                do not repeat moves
            more general from next games
                assume repeated action sequences have same reward
                    do not explore them
                similar action sequences different though


method
    play 10 episodes
        high exploration
        overfit to these experiences
            store whole game until good reward or done
            adjust rewards, values etc
            repeated random sample
    start training
        high exploration until find good reward
        duplicate paths with high exploraton at end?

    reward function = fn(reward) + step
        encourage longer games
            once longest game reached then it will learn optimal policy

    learns using a sequence of actions
        dynamic lstm??
        something better?

    limit to exploration
        if no reward from exploration path stop going down that path

    add "boredom"
        agent doent want to do something with no result/repeat for too long

    generalized overfitting
        see one picture of cat from front
        overfit and generalize by creating more input from the one sample
    
    at start intermediate net pulls weights from long net 
        (long net predicts weights?)
        (long just stores a state, reward, weights)
        (predicts which one based on state similarities)


--------------------------------------------------------------------------




net focuses
    fast training
        use overfitting and long term mem weights
            good init
            fast exploration
    better learning
        think about choice
    structure
        immediate mem --> overfits to recent experiences
        short term mem --> models local task / env
        long term mem --> learns all dependecies / patterns

net goals
    better learning
        learns why to do something
        learns when to trust immediate, short, long mem outputs

ideas
    one model (no reward input?)
        extract (action rewards, state reward?) from state
            defaults to length of time per episode if no reward given
                able to switch between learned, given, default
            self tunes to focus on that reward
        extract best action
            explore more if high entropy
        evolve self
            able to change structure and self tune
        predict step to convergence
            ie one large step then learn with small steps
            aka recognize activity and remember weights?
